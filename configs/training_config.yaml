# Training configuration with energy efficiency settings
training:
  # Basic training parameters
  model_name_or_path: "EleutherAI/pythia-410m"  # Start with smaller model for testing
  dataset_name: "wikitext"
  dataset_config_name: "wikitext-103-raw-v1"
  max_seq_length: 512
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 16
  learning_rate: 5e-5
  weight_decay: 0.01
  num_train_epochs: 3
  warmup_steps: 500
  logging_steps: 100
  evaluation_strategy: "steps"
  eval_steps: 500
  save_steps: 1000
  fp16: true
  
  # Energy efficiency parameters
  energy_monitoring_interval: 60  # seconds
  energy_efficiency_threshold: 0.75  # ratio of peak efficiency
  adaptive_precision: true
  dynamic_layer_freezing: true
  min_active_layers: 3
  layer_activation_threshold: 0.01  # gradient magnitude threshold

# DeepSpeed ZeRO-3 config
deepspeed:
  zero_optimization:
    stage: 3
    offload_optimizer:
      device: "cpu"
      pin_memory: true
    offload_param:
      device: "cpu"
      pin_memory: true
    overlap_comm: true
    contiguous_gradients: true
    reduce_bucket_size: 5e7
    stage3_prefetch_bucket_size: 5e7
    stage3_param_persistence_threshold: 1e5
    sub_group_size: 1e9
    stage3_max_live_parameters: 1e9
    stage3_max_reuse_distance: 1e9
    stage3_gather_16bit_weights_on_model_save: true
  
  # Adaptive parameters for energy efficiency
  adaptive_training:
    enabled: true
    energy_threshold_high: 250  # watts
    energy_threshold_low: 150   # watts
    initial_thread_percentage: 50
    min_thread_percentage: 30
    max_thread_percentage: 80
    adaptation_cooldown: 300    # seconds

# MPS configuration
mps:
  enabled: true
  initial_thread_percentage: 50
  shared_resources: 2  # Number of training processes per GPU